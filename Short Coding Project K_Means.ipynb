{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short Coding Project: K-Means Clustering\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "You will perform **K-Means clustering** to group **heavy equipment usage scenarios** based on multiple features. The dataset is synthetic, focusing on equipment rental costs, usage hours, maintenance issues, transport distances, and more. Tasks include:\n",
    "\n",
    "1. **Generating a synthetic dataset** (already in the code) with random missing values.\n",
    "2. **Handling missing data** appropriately.\n",
    "3. **Encoding categorical variables** using LabelEncoder.\n",
    "4. **Scaling features** for distance-based clustering.\n",
    "5. **Determining the optimal number of clusters** via the elbow method.\n",
    "6. **Visualizing and interpreting** the clustering results.\n",
    "7. **(Advanced)** Optionally using the Silhouette Score for deeper evaluation.\n",
    "\n",
    "- Delete the `# YOUR CODE HERE` comments and write your code.\n",
    "- **Do not change** the variable names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Imports & Synthetic Dataset Generation\n",
    "\n",
    "Run the cell below to import necessary libraries and create a **synthetic dataset** of 200 samples representing heavy equipment usage. Some values are intentionally replaced with `NaN` to simulate missing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "num_samples = 200\n",
    "\n",
    "# Generate three features using make_blobs\n",
    "blobs, _ = make_blobs(n_samples=num_samples, n_features=3, centers=4, random_state=42)\n",
    "\n",
    "# Define a function to linearly scale a feature to a desired range.\n",
    "def scale_feature(feature, new_min, new_max):\n",
    "    old_min = feature.min()\n",
    "    old_max = feature.max()\n",
    "    return new_min + (feature - old_min) * (new_max - new_min) / (old_max - old_min)\n",
    "\n",
    "# Transform the blob columns:\n",
    "rental_cost = scale_feature(blobs[:, 0], 2000, 7000).round(2)\n",
    "usage_hours = scale_feature(blobs[:, 1], 60, 180).round(1)\n",
    "maintenance_issues = scale_feature(blobs[:, 2], 0, 10)\n",
    "maintenance_issues = np.round(maintenance_issues).astype(int)\n",
    "\n",
    "# Generate Transport_Distance (in miles)\n",
    "transport_distance = np.random.randint(low=10, high=1000, size=num_samples)\n",
    "\n",
    "# Generate categorical columns\n",
    "equipment_type = np.random.choice(['Excavator','Crane','Bulldozer','Loader','Forklift'], size=num_samples)\n",
    "project_location = np.random.choice(['Urban','Rural','Suburban'], size=num_samples)\n",
    "fuel_type = np.random.choice(['Diesel','Electric','Gas'], size=num_samples)\n",
    "\n",
    "# Create the DataFrame with the original feature names\n",
    "data = pd.DataFrame({\n",
    "    'Rental_Cost': rental_cost,\n",
    "    'Usage_Hours': usage_hours,\n",
    "    'Maintenance_Issues': maintenance_issues,\n",
    "    'Transport_Distance': transport_distance,\n",
    "    'Equipment_Type': equipment_type,\n",
    "    'Project_Location': project_location,\n",
    "    'Fuel_Type': fuel_type\n",
    "})\n",
    "\n",
    "# Introduce some missing values randomly (~10% chance per value)\n",
    "mask_missing = np.random.choice([True, False], size=data.shape, p=[0.1, 0.9])\n",
    "data = data.mask(mask_missing)\n",
    "\n",
    "print(\"Initial data preview:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset columns**:\n",
    "\n",
    "1. **Rental_Cost (float)** — Daily cost to rent the equipment (USD).  \n",
    "2. **Usage_Hours (float)** — Total equipment operating hours logged.  \n",
    "3. **Maintenance_Issues (int)** — Count of maintenance issues reported.  \n",
    "4. **Transport_Distance (int)** — Distance in miles transported to/from the site.  \n",
    "5. **Equipment_Type (object)** — Type of heavy equipment (Excavator, Crane, etc.).  \n",
    "6. **Project_Location (object)** — Where the project is located (Urban, Rural, or Suburban).  \n",
    "7. **Fuel_Type (object)** — Type of fuel (Diesel, Electric, or Gas).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Handling Missing Values\n",
    "\n",
    "1. **Create** a variable `missing_counts` that counts missing values per column.\n",
    "2. **Fill** missing values for **numeric** columns with their **mean**.\n",
    "3. **Fill** missing values for **categorical** columns with `'Unknown'`.\n",
    "4. **Create** a variable `post_missing_counts` that stores the count of missing values for each column after filling. This should confirm that all missing values have been handled (i.e., should be 0 for all columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create `missing_counts`\n",
    "missing_counts =  # YOUR CODE HERE\n",
    "print(\"Missing values before filling:\\n\", missing_counts, \"\\n\")\n",
    "\n",
    "# 2. Fill numeric columns with the mean\n",
    "#    Hint: data.select_dtypes(include=[np.number]) -> numeric columns\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# 3. Fill categorical columns with 'Unknown'\n",
    "#    Hint: data.select_dtypes(include=['object']) -> categorical columns\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# 4. Create `post_missing_counts`\n",
    "post_missing_counts = # YOUR CODE HERE\n",
    "print(\"Missing values after filling:\\n\", post_missing_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Encoding Categorical Features\n",
    "\n",
    "K-Means requires numeric data. We will encode `Equipment_Type`, `Project_Location`, and `Fuel_Type` using `LabelEncoder`. Here we Identified the list of columns to encode and stored it in `categorical_cols`.\n",
    "\n",
    "- You need to **initialize** a LabelEncoder for each and **transform** the columns in `data`.\n",
    "\n",
    "At the end we print the first 5 rows of the updated DataFrame to verify changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1. Identify the categorical columns\n",
    "categorical_cols = [\"Equipment_Type\", \"Project_Location\", \"Fuel_Type\"]\n",
    "\n",
    "# 2. Encode these columns\n",
    "for col in categorical_cols:\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "# 3. Print first 5 rows to check\n",
    "print(\"\\nData after label encoding (first 5 rows):\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Feature Scaling\n",
    "\n",
    "We will use **all columns** in `data` for clustering. Since K-Means uses distance metrics, standardizing helps prevent bias from differing scales.\n",
    "\n",
    "1. **Copy** the entire `data` into a variable `features`.\n",
    "2. **Initialize** a `StandardScaler` as `scaler` and fit-transform it on `features`. Store the result in `features_scaled`.\n",
    "\n",
    "Now we print the shape of `features_scaled` to confirm it has 200 rows and 7 columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Copy data\n",
    "features = # YOUR CODE HERE\n",
    "\n",
    "# 2. Scale the features\n",
    "scaler =  # YOUR CODE HERE\n",
    "features_scaled = # YOUR CODE HERE\n",
    "\n",
    "# Print shape of features_scaled\n",
    "print(\"Shape of scaled features:\", features_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Elbow Method\n",
    "\n",
    "We will decide the optimal number of clusters `k` by looking at **inertia** (sum of squared distances to cluster centers) for `k` in `[1..10]`.\n",
    "\n",
    "First, we initialize a list `inertia_values`.\n",
    "\n",
    "Now, you need to do these steps:\n",
    "\n",
    "- **Loop** through each `k` in `[1..10]`.\n",
    "- **Create** a `KMeans`, with these settings: `init='k-means++'`, `n_init=10`, `random_state=42`.\n",
    "- **Fit** it to `features_scaled`.\n",
    "- **Append** `kmeans.inertia_` to `inertia_values`.\n",
    "\n",
    "At the end, we plot `k` vs. `inertia_values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia_values = []\n",
    "k_values = range(1, 11)\n",
    "\n",
    "for k in k_values:\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.figure()\n",
    "plt.plot(k_values, inertia_values, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Choose** a value for `k` as `k=3`, based on the identified \"elbow\" point in the plot.\n",
    "\n",
    "### Question 5: K-Means Clustering & Visualization\n",
    "\n",
    "1. **Set** `optimal_k` to the chosen value above.\n",
    "2. **Create** a `KMeans` model (`kmeans_model`) , with these settings: `init='k-means++'`, `n_init=10`, `random_state=42`. Then, fit it to `features_scaled`, and get cluster labels in `labels`.\n",
    "3. **Add** these labels to the original `data` in a new column named `\"Cluster\"`.\n",
    "4. **Plot** the clusters in 3D using `Rental_Cost`, `Usage_Hours`, and `Maintenance_Issues` colored by `\"Cluster\"`.  \n",
    "\n",
    "   - Also **plot** cluster centroids by inverse-transforming the model’s `cluster_centers_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set `optimal_k`\n",
    "optimal_k = # YOUR CODE HERE\n",
    "\n",
    "# 2. Create and fit KMeans\n",
    "kmeans_model = # YOUR CODE HERE\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# 3. Assign labels to `data[\"Cluster\"]`\n",
    "labels = # YOUR CODE HERE\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# 4. Plot the clusters\n",
    "centroids_scaled = # YOUR CODE HERE\n",
    "centroids_original = # YOUR CODE HERE\n",
    "\n",
    "# Create a 3D scatter plot using the 3 blob-generated features:\n",
    "# Rental_Cost, Usage_Hours, and Maintenance_Issues\n",
    "fig = plt.figure(figsize=(8, 30))\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.view_init(elev=25, azim=70)\n",
    "scatter = ax.scatter(data[\"Rental_Cost\"], data[\"Usage_Hours\"], data[\"Maintenance_Issues\"],\n",
    "                     c=data[\"Cluster\"], cmap='viridis', alpha=0.6)\n",
    "ax.set_xlabel(\"Rental_Cost\", fontsize=12)\n",
    "ax.set_ylabel(\"Usage_Hours\", fontsize=12)\n",
    "ax.set_zlabel(\"Maintenance_Issues\", fontsize=12)\n",
    "\n",
    "plt.title(\"3D K-Means Clusters\", fontsize=14)\n",
    "\n",
    "# Plot centroids\n",
    "ax.scatter(centroids_original[:, 0], centroids_original[:, 1], centroids_original[:, 2],\n",
    "           c='red', marker='X', s=200, label='Centroids')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 (Advanced): Silhouette Score\n",
    "\n",
    "**Silhouette Score** measures how similar each sample is to its own cluster compared to other clusters. Values range from `-1` to `1`, and higher generally indicates better clustering.\n",
    "\n",
    "1. **Import** `silhouette_score` from `sklearn.metrics`.\n",
    "2. **Compute** the silhouette score using `(features_scaled, labels)` and store it in `sil_score`.\n",
    "3. **Print** `sil_score`.\n",
    "4. **Loop** loop over different `k` values to find which has the highest silhouette score.\n",
    "\n",
    "**Hint**: Check [Silhouette Score Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import silhouette_score\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# 2. Compute silhouette score\n",
    "sil_score = # YOUR CODE HERE\n",
    "print(\"\\nSilhouette Score:\", sil_score)\n",
    "\n",
    "# 3. Loop over range of k and see which yields best silhouette\n",
    "best_k = None\n",
    "best_score = -1\n",
    "for k_test in range(2, 11):\n",
    "    km_test = KMeans(n_clusters=k_test, init='k-means++', n_init=10, random_state=42)\n",
    "    km_test.fit(features_scaled)\n",
    "    test_labels = km_test.labels_\n",
    "    score = # YOUR CODE HERE\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_k = k_test\n",
    "    print(f\"k={k_test}, Silhouette Score={score:.4f}\")\n",
    "print(f\"Best k by silhouette: {best_k}, Score={best_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
