{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Coding Project: Decision Trees Classification\n",
    "\n",
    "#### Project Overview\n",
    "\n",
    "In this project, you will apply decision tree classification to predict the condition of culverts based on various environmental and physical attributes using the Augmented Culvert Dataset. You will preprocess the data, perform feature engineering, build and evaluate decision tree models, and explore advanced topics such as feature importance and hyperparameter tuning.\n",
    "\n",
    "- Delete the `# YOUR CODE HERE` comments and write your code.\n",
    "- **Do not change** the variable names.\n",
    "\n",
    "### Load the Dataset\n",
    "\n",
    "Start by loading the Augmented Culvert Dataset and examining its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/CyConProject/Lab/main/Datasets/Augmented%20Culvert%20Dataset.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Data Exploration\n",
    "\n",
    "Explore the dataset to understand its structure and content.\n",
    "\n",
    "1. Display the summary statistics of the numerical columns in the dataset.\n",
    "2. Identify and print the names of categorical columns in the dataset.\n",
    "\n",
    "**Hint for Part 2**: You can use the `dtypes` attribute of the DataFrame to check the data type of each column. Categorical columns often have the data type `'object'`, which means they contain text data. You can loop through all the columns and collect the names of columns where the data type is `'object'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics of numerical columns\n",
    "des = # YOUR CODE HERE\n",
    "print(des)\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_columns = []\n",
    "for col in df.columns:\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "# Display the categorical columns\n",
    "print(\"Categorical Columns:\", categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Handle Missing Values\n",
    "\n",
    "Handle missing values in the dataset.\n",
    "\n",
    "1. **Identify columns with missing values** and the number of missing entries in each.\n",
    "2. **Fill the missing values** in the `'Flooding_Frequency'` column with the mode (most frequent value).\n",
    "3. **Verify that there are no more missing values** in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns with missing values\n",
    "missing_values = # YOUR CODE HERE\n",
    "print(\"Missing Values:\\n\", missing_values)\n",
    "\n",
    "# Fill missing values of 'Flooding_Frequency' column with mode\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Verify that there are no more missing values\n",
    "missing_values_after = # YOUR CODE HERE\n",
    "print(\"Missing Values After Filling:\\n\", missing_values_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Feature Engineering\n",
    "\n",
    "Feature engineering is a crucial step in improving a model's performance because it allows us to create new variables that may reveal patterns or relationships not captured by the original features. By creating features like `'Age_Category'` and `'Length_to_Age_Ratio'`, we can simplify complex relationships, such as the effect of age and length, and make them more understandable to the model, potentially leading to better predictions.\n",
    "\n",
    "1. Create a new feature `'Age_Category'` by binning the `'Age'` column into three categories: `'New'` (<=10 years), `'Moderate'` (11-30 years), and `'Old'` (>30 years).\n",
    "2. Create a new feature `'Length_to_Age_Ratio'` by dividing `'length'` by `'Age'`.\n",
    "\n",
    "**Hint:** When creating the `'Length_to_Age_Ratio'` feature, make sure to handle cases where `'Age'` might be zero to avoid division by zero errors. You need to replace such values with a default (e.g., 0) to ensure the model runs smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'Age_Category' feature\n",
    "def categorize_age(age):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "df['Age_Category'] = df['Age'].apply(categorize_age)\n",
    "\n",
    "# Create 'Length_to_Age_Ratio' feature\n",
    "df['Length_to_Age_Ratio'] = # YOUR CODE HERE\n",
    "\n",
    "# Handle division by zero if any\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Encode Categorical Variables\n",
    "\n",
    "Machine learning algorithms require numerical input data. Therefore, we need to encode categorical variables into numerical form. One common method is **one-hot encoding**, which converts categorical variables into a set of binary columns, each representing a unique category with 1s and 0s.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Update the list of categorical columns by including the new `'Age_Category'` feature created in Question 3.\n",
    "2. Use `pd.get_dummies()` to perform one-hot encoding on these categorical columns, ensuring the encoded columns are in integer format (0s and 1s).\n",
    "\n",
    "**Hint:** Use the `columns` parameter in `pd.get_dummies()` to specify the columns you want to encode and set `dtype` to ensure the encoded columns are integers. You can learn more in the [Pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the list of categorical columns to include 'Age_Category'\n",
    "updated_categorical_columns = # YOUR CODE HERE\n",
    "\n",
    "# One-hot encode the categorical columns\n",
    "df_encoded = # YOUR CODE HERE\n",
    "\n",
    "# Display the first few rows of the encoded dataset\n",
    "df_encoded.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Split the Data into Training and Testing Sets\n",
    "\n",
    "Split the dataset into training and testing sets.\n",
    "\n",
    "- Use 75% of the data for training and 25% for testing.\n",
    "- Set `random_state=42` for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df_encoded.drop('Cul_rating', axis=1)\n",
    "y = df_encoded['Cul_rating']\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = # YOUR CODE HERE\n",
    "\n",
    "# Print the shapes of X_train and y_train\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: Train and Evaluate the Decision Tree Classifier\n",
    "\n",
    "Initialize and train a decision tree classifier with customizable parameters to control overfitting.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Initialize the classifier with `random_state=42`, and modify the following parameters to prevent overfitting:\n",
    "   - `max_depth=10`: This limits the depth of the tree, ensuring the model doesn't become too complex.\n",
    "   - `min_samples_split=10`: This ensures that a node must have at least 10 samples before it can be split, reducing overfitting by preventing small, overly specific splits.\n",
    "   - `min_samples_leaf=5`: This ensures that each leaf node has at least 5 samples, which helps smooth the model's predictions and avoid capturing noise in the training data.\n",
    "2. Train the classifier on the training data.\n",
    "3. Evaluate the model by calculating both the training and test accuracy scores.\n",
    "4. Print the classification report for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the Decision Tree Classifier with additional parameters\n",
    "decision_tree = # YOUR CODE HERE\n",
    "\n",
    "# Train the classifier\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = # YOUR CODE HERE\n",
    "\n",
    "# Make predictions on the training data\n",
    "y_train_pred = # YOUR CODE HERE\n",
    "\n",
    "# Calculate the accuracy scores\n",
    "train_accuracy = # YOUR CODE HERE\n",
    "test_accuracy = # YOUR CODE HERE\n",
    "\n",
    "# Print the classification report for the test data\n",
    "report = # YOUR CODE HERE\n",
    "\n",
    "# Display the results\n",
    "print(f\"Training Accuracy of the Decision Tree Classifier: {train_accuracy:.2f}\")\n",
    "print(f\"Test Accuracy of the Decision Tree Classifier: {test_accuracy:.2f}\")\n",
    "print(\"Classification Report for Test Data:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: Analyze Feature Importance (Advanced)\n",
    "\n",
    "Feature importance helps identify which features have the most influence on the prediction of the target variable. Understanding feature importance can provide insights into the data and help in feature selection and model improvement.\n",
    "\n",
    "**Hint**: Use the `feature_importances_` attribute of the trained decision tree model to get the importance of each feature. You can learn more about feature importance in the [scikit-learn documentation](https://scikit-learn.org/dev/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.feature_importances_).\n",
    "\n",
    "Extract the feature importances from the trained model. You can see a bar plot showing the top 10 most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract feature importances\n",
    "importances = # YOUR CODE HERE\n",
    "\n",
    "# Get indices of the top 10 features\n",
    "indices = np.argsort(importances)[-10:]\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Top 10 Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], align='center')\n",
    "plt.yticks(range(len(indices)), [X_train.columns[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
