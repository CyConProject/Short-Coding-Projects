{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Coding Project: Logistic Regression\n",
    "\n",
    "#### Project Overview\n",
    "\n",
    "In this project, you will apply logistic regression to predict whether culverts need repair based on various environmental and physical attributes using the Augmented Culvert Dataset. You will preprocess the data, handle categorical variables, perform feature scaling, apply feature selection, build and evaluate a logistic regression model, and explore advanced topics such as ROC curves and experimenting with different solvers.\n",
    "\n",
    "- Delete the `# YOUR CODE HERE` comments and write your code.\n",
    "- **Do not change** the variable names.\n",
    "\n",
    "### Load the Dataset\n",
    "\n",
    "Start by loading the Augmented Culvert Dataset and examining its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/CyConProject/Lab/main/Datasets/Augmented%20Culvert%20Dataset.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Data Exploration and Preprocessing\n",
    "\n",
    "1. **Identify columns with missing values** and the number of missing entries in each.\n",
    "\n",
    "2. **Handle missing values** in the dataset:\n",
    "\n",
    "   - For the `'Flooding_Frequency'` column, replace missing values with `'No Flooding'`.\n",
    "\n",
    "3. **Convert the `'Cul_rating'` column to a binary target variable** where ratings of 0 or 1 are mapped to `0` (needs repair), and ratings of 2, 3, or 4 are mapped to `1` (satisfactory to good condition).\n",
    "\n",
    "4. **Display the number of instances in each class** of the binary target variable.\n",
    "\n",
    "**Hint for Part 2**: Use the `fillna()` method with a specified value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns with missing values\n",
    "missing_values_count = # YOUR CODE HERE\n",
    "print(\"Missing Values:\\n\", missing_values_count)\n",
    "\n",
    "# Fill missing values in 'Flooding_Frequency' with 'No Flooding'\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Convert 'Cul_rating' to binary target variable\n",
    "def convert_rating(rating):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "data['Cul_rating'] = data['Cul_rating'].apply(convert_rating)\n",
    "\n",
    "# Display the number of instances in each class\n",
    "class_counts = # YOUR CODE HERE\n",
    "\n",
    "cleaned_data = data.copy()\n",
    "print(\"Class Counts:\\n\", class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Encode Categorical Variables\n",
    "\n",
    "Machine learning algorithms require numerical input data. Therefore, we need to encode categorical variables into numerical form. One common method is **one-hot encoding**, which converts categorical variables into a set of binary columns, each representing a unique category with 1s and 0s.\n",
    "\n",
    "1. **Identify the categorical columns** that need to be encoded.\n",
    "\n",
    "2. **Encode the categorical variables** using one-hot encoding (use `pd.get_dummies()`).\n",
    "\n",
    "3. **Display the first few rows** of the updated dataset to verify the changes.\n",
    "\n",
    "\n",
    "**Hint for part 1:** When identifying categorical columns, you can use `data.select_dtypes(include=['object'])` to select columns that are recognized as categorical in Pandas. This method efficiently identifies columns with non-numeric data, making it easier to encode them correctly. If you want to know more about this method, refer to the [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html).\n",
    "\n",
    "**Hint for part 2:** Use the `columns` parameter in `pd.get_dummies()` to specify the columns you want to encode and set `drop_first=True` to drop the first category of each variable. Dropping the first category helps avoid redundancy since the remaining columns provide all the necessary information. Also, don't forget to set `dtype` to ensure the encoded columns are integers. You can learn more in the [Pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_columns = # YOUR CODE HERE\n",
    "\n",
    "# Encode categorical variables using one-hot encoding\n",
    "data_encoded = # YOUR CODE HERE\n",
    "\n",
    "# Display the updated dataset\n",
    "data_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Feature Scaling\n",
    "\n",
    "We will perform feature scaling to ensure that all features contribute equally to the model.\n",
    "\n",
    "1. **Normalize the feature data** using `MinMaxScaler`.\n",
    "\n",
    "2. **Display the shape of the feature matrix** after scaling.\n",
    "\n",
    "**Note**: Scaling is important for logistic regression to ensure convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data_encoded.drop('Cul_rating', axis=1)\n",
    "y = data_encoded['Cul_rating']\n",
    "\n",
    "# Normalize the feature data\n",
    "scaler = # YOUR CODE HERE\n",
    "X_scaled = # YOUR CODE HERE\n",
    "X_scaled_shape = # YOUR CODE HERE\n",
    "\n",
    "# Display the shape of X_scaled\n",
    "print(\"Shape of X_scaled:\", X_scaled_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Feature Selection\n",
    "\n",
    "1. **Use SelectKBest** with the chi-squared (`chi2`) statistical test to select the top 15 features from the scaled dataset that are most relevant to the target variable. For more details, refer to the [SelectKBest Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html).\n",
    "\n",
    "\n",
    "2. **Reduce the training and testing data** to these selected features.\n",
    "\n",
    "3. **Display the names of the selected features**.\n",
    "\n",
    "**Hint**: Use `SelectKBest` from `sklearn.feature_selection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Apply SelectKBest with chi-squared test\n",
    "selector = # YOUR CODE HERE\n",
    "X_selected = # YOUR CODE HERE\n",
    "\n",
    "# Get the boolean mask of selected features\n",
    "mask = # YOUR CODE HERE\n",
    "\n",
    "# Get the feature names\n",
    "selected_features = # YOUR CODE HERE\n",
    "\n",
    "# Display selected feature names\n",
    "print(\"Selected Features:\\n\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data into Training and Testing Sets\n",
    "\n",
    "As you can see, we splitted the dataset into training and testing sets. We used 80% of the data for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_selected, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Print the shapes\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Train and Evaluate the Logistic Regression Model\n",
    "\n",
    "1. **Initialize** the logistic regression model with default parameters. Set the `max_iter=1000`.\n",
    "\n",
    "2. **Train** the model on the selected features from the training data.\n",
    "\n",
    "3. **Calculate the accuracy scores** for both the training and testing sets.\n",
    "\n",
    "4. **Print the classification report** for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = # YOUR CODE HERE\n",
    "\n",
    "# Train the model on selected features\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Make predictions on training and testing data\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy scores\n",
    "train_accuracy = # YOUR CODE HERE\n",
    "test_accuracy = # YOUR CODE HERE\n",
    "\n",
    "# Print classification report for test data\n",
    "report = # YOUR CODE HERE\n",
    "\n",
    "# Display the results\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: Advanced Evaluation Metrics\n",
    "\n",
    "The ROC (Receiver Operating Characteristic) curve is a graph that shows the performance of a classification model at all classification thresholds. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR). The AUC (Area Under the Curve) quantifies the overall ability of the model to distinguish between classes. A higher AUC indicates better model performance.\n",
    "\n",
    "Please watch [this video](https://www.youtube.com/watch?v=4jRBRDbJemM&t=907s) to understand the ROC curve and AUC in detail. Additionally, read the official [scikit-learn documentation](https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.roc_auc_score.html) to learn more about the ROC AUC score.\n",
    "\n",
    "\n",
    "**Calculate the ROC AUC score** for the test predictions. Then, you can see the ROC curve plot.\n",
    "\n",
    "**Hint**: Use `roc_auc_score` from `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate probabilities for ROC curve\n",
    "y_test_probability = # YOUR CODE HERE\n",
    "\n",
    "# Calculate ROC AUC score\n",
    "roc_auc_sco = # YOUR CODE HERE\n",
    "print(\"ROC AUC Score:\", roc_auc_sco)\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_probability)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % roc_auc_sco)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: Experiment with Different Solvers (Advanced)\n",
    "\n",
    "Logistic regression solvers are algorithms that optimize the model's parameters. Here are some examples of the solvers:\n",
    "\n",
    "- **`'liblinear'`**: A good choice for small datasets, using a coordinate descent algorithm.\n",
    "- **`'saga'`**: Effective for large datasets and supports L1 and L2 regularization.\n",
    "- **`'lbfgs'`**: Uses the Limited-memory Broyden–Fletcher–Goldfarb–Shanno algorithm, efficient for multiclass problems.\n",
    "- **`'newton-cg'`**: An iterative solver that approximates the Newton-Raphson method, well-suited for larger datasets.\n",
    "\n",
    "**Documentation:** You can read more in the [Logistic Regression Solvers Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "\n",
    "1. **Train logistic regression models** using different solvers (`'liblinear'`, `'saga'`, `'lbfgs'`, `'newton-cg'`).\n",
    "\n",
    "2. **Evaluate each model** by calculating the test accuracy and ROC AUC score.\n",
    "\n",
    "3. **Create a comparison table** showing the solver used, test accuracy, and ROC AUC score.\n",
    "\n",
    "**Hint:** Set `max_iter=1000` to allow sufficient iterations for the solvers to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solvers = ['liblinear', 'saga', 'lbfgs', 'newton-cg']\n",
    "results = []\n",
    "\n",
    "for solver in solvers:\n",
    "    # Initialize and train the model\n",
    "    model = # YOUR CODE HERE\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_test_pred = # YOUR CODE HERE\n",
    "    y_test_proba = # YOUR CODE HERE\n",
    "    accuracy = # YOUR CODE HERE\n",
    "    roc_auc = # YOUR CODE HERE\n",
    "    \n",
    "    # Append results\n",
    "    results.append({\n",
    "        'Solver': # YOUR CODE HERE\n",
    "        'Test Accuracy': # YOUR CODE HERE\n",
    "        'ROC AUC Score': # YOUR CODE HERE\n",
    "    })\n",
    "    print(f\"Solver: {solver}, Test Accuracy: {accuracy:.2f}, ROC AUC Score: {roc_auc:.2f}\")\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nComparison of Different Solvers:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason all the outputs are the same across different solvers is because **logistic regression is a convex optimization problem with a single global minimum**. This means that regardless of the solver used, they all aim to minimize the same cost function and, given enough iterations and proper convergence, will find the same optimal solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
